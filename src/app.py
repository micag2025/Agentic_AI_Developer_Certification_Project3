
# app.py

"""
Streamlit UI for comparing scientific publications.
"""

# ğŸ§  STREAMLIT APP FOR PUBLICATION COMPARATOR

import os
import json
from datetime import datetime
from html import escape
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv

#from paths import SRC_DIR
#src = SRC_DIR

src_dir = Path("src")
from paths import SRC_DIR

import sys
sys.path.insert(0, str(SRC_DIR.parent))

#from src.paths import SAMPLE_PUBLICATION_DIR, COMPARISONS_DIR, OUTPUTS_DIR, LOGS_DIR, PROFILES_DIR


from explorer import PublicationExplorer
from src.paths import SAMPLE_PUBLICATION_DIR, COMPARISONS_DIR, OUTPUTS_DIR, LOGS_DIR, PROFILES_DIR



# ğŸŒ Load .env configuration
load_dotenv()

# ğŸ” API keys
openai_key = os.getenv("OPENAI_API_KEY", "")
tavily_key = os.getenv("TAVILY_API_KEY", "")

# ğŸ“± Streamlit UI Setup
st.set_page_config(page_title="Publication Comparator", layout="wide")
st.title("ğŸ“Š AI-Powered Ready Tensor Publication Comparator")
st.markdown("""
Simply select two `.txt` publication files and a query type to compare:

- Tools  
- Tasks  
- Datasets  
- Evaluation Methods  
- Results  

---

ğŸ§  **The backend ensures**:
- Structured extraction  
- Robust validation  
- Traceable output
""")

# ğŸ“‚ Sidebar Info
with st.sidebar:
    st.markdown("## ğŸ¤– About this App")
    st.markdown("""
This AI-powered comparison tool uses:

- ğŸ”— **LangChain** for LLM orchestration  
- ğŸ’¬ **OpenAI** for LLM completions  
- ğŸ” **Tavily** for web-based fact checks  
- ğŸ–¥ **Streamlit** for interactive visualization  

---
### ğŸ“ Output Locations
- ğŸ§ª Validated profiles â†’ `outputs/profiles/`
- ğŸ”„ Comparisons â†’ `outputs/comparisons/`
- ğŸ“ Logs â†’ `logs/`
---
""")

    # Directories
    profiles_dir = PROFILES_DIR
    comparisons_dir = COMPARISONS_DIR
    logs_dir = LOGS_DIR

    # âœ… Latest Validated Profile
    latest_profile = sorted(profiles_dir.glob("validated_profile_pub*.json"), reverse=True)
    if latest_profile:
        with open(latest_profile[0], "rb") as f:
            st.download_button("â¬‡ï¸ Download Latest Validated Profile", f, file_name=latest_profile[0].name)
    else:
        st.info("âš ï¸ No validated profiles saved yet.")

    # âœ… Latest Comparison
    latest_cmp = sorted(comparisons_dir.glob("comparison_*.json"), reverse=True)
    if latest_cmp:
        with open(latest_cmp[0], "rb") as f:
            st.download_button("â¬‡ï¸ Download Latest Comparison", f, file_name=latest_cmp[0].name)
    else:
        st.info("âš ï¸ No comparisons saved yet.")

    # âœ… Latest Log File
    latest_log = sorted(logs_dir.glob("*.log"), reverse=True)
    if latest_log:
        with open(latest_log[0], "rb") as f:
            st.download_button("â¬‡ï¸ Download Latest Log", f, file_name=latest_log[0].name)
    else:
        st.info("âš ï¸ No logs found yet.")


# ğŸ“„ Load publications
pub_dir = Path(SAMPLE_PUBLICATION_DIR)
pub_files = sorted(f.name for f in pub_dir.glob("*.txt"))
pub1 = st.selectbox("Select Publication 1", [""] + pub_files, key="pub1")
pub2 = st.selectbox("Select Publication 2", [""] + pub_files, key="pub2")

# ğŸ§  Query Selection
query_options = [""] + [
    "Tool Usage", "Evaluation Methods", "Task Types", "Datasets", "Results", "Other (custom)"
]
query_choice = st.selectbox("Select a query type", query_options)

user_query = ""
if query_choice == "Other (custom)":
    user_query = st.text_input("Type your custom query:")
elif query_choice:
    user_query = query_choice

# ğŸš€ Comparison Trigger
if st.button("ğŸš€ Run Comparison"):
    if not pub1 or not pub2:
        st.warning("Please select both publications before running the comparison.")
    elif not user_query:
        st.warning("Please enter or select a valid query.")
    else:
        explorer = PublicationExplorer()
        state = {
            "pub1_path": str(pub_dir / pub1),
            "pub2_path": str(pub_dir / pub2),
            "user_query": user_query,
            "pub1_profile": "",
            "pub2_profile": "",
            "comparison": "",
            "trends": "",
            "summary": "",
            "fact_check": "",
            "extra_info": "",
            "lnode": "",
            "count": 0
        }

        with st.spinner("ğŸ” Processing publications... This may take a moment."):
            result = explorer.graph.invoke(state)

        # âœ… Always save validated profiles
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if result.get("pub1_profile"):
            profile_path1 = PROFILES_DIR / f"validated_profile_pub1_{Path(pub1).stem}_{timestamp}.json"
            with open(profile_path1, "w", encoding="utf-8") as f:
                json.dump(result["pub1_profile"], f, indent=2, ensure_ascii=False)

        if result.get("pub2_profile"):
            profile_path2 = PROFILES_DIR / f"validated_profile_pub2_{Path(pub2).stem}_{timestamp}.json"
            with open(profile_path2, "w", encoding="utf-8") as f:
                json.dump(result["pub2_profile"], f, indent=2, ensure_ascii=False)

        # âœ… Display Results
        st.subheader("âœ… Summary")
        st.text_area("Summary", result.get("summary", "[No summary]"), height=300)

        with st.expander("ğŸ“˜ Fact Check"):
            st.text_area("Fact Check", result.get("fact_check", "[No fact check]"), height=300)

        with st.expander("ğŸ§  Enrichment"):
            st.text_area("ReAct Agent Output", result.get("extra_info", "[No enrichment]"), height=300)

        # ğŸ“ Save Comparison Results
        base_name = f"comparison_{Path(pub1).stem}_vs_{Path(pub2).stem}_{timestamp}"
        json_path = COMPARISONS_DIR / f"{base_name}.json"
        html_path = COMPARISONS_DIR / f"{base_name}.html"

        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)

            # Save HTML report
            html_report = f"""
            <!DOCTYPE html>
            <html>
            <head><meta charset="UTF-8"><title>Comparison</title></head>
            <body>
                <h1>ğŸ“Š Comparison</h1>
                <p><strong>Query:</strong> {escape(user_query)}</p>
                <h2>âœ… Summary</h2><pre>{escape(result.get("summary", "[No summary]"))}</pre>
                <h2>ğŸ“˜ Fact Check</h2><pre>{escape(result.get("fact_check", "[No fact check]"))}</pre>
                <h2>ğŸ§  Enrichment</h2><pre>{escape(result.get("extra_info", "[No enrichment]"))}</pre>
            </body>
            </html>
            """
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(html_report)

            # Get latest log
            log_files = sorted(LOGS_DIR.glob("*.log"), reverse=True)
            latest_log = log_files[0].name if log_files else "No log file found"

            # âœ… Display success message
            st.success(
                f"ğŸ“ Results saved:\n"
                f"â€¢ ğŸ§ª Comparison JSON: `{json_path.name}`\n"
                f"â€¢ ğŸ“„ Comparison HTML: `{html_path.name}`\n"
                f"â€¢ âœ… Profiles: saved in `outputs/profiles/`\n"
                f"â€¢ ğŸ—’ Logs: `{latest_log}`"
            )

        except Exception as e:
            st.error(f"âŒ Failed to save results: {e}")
